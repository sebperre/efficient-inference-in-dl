nohup: ignoring input
Training on GPU...
Iteration 1: Epoch [1/50], Loss: 1.8568
Iteration 1: Epoch [2/50], Loss: 1.4657
Iteration 1: Epoch [3/50], Loss: 1.3312
Iteration 1: Epoch [4/50], Loss: 1.2410
Iteration 1: Epoch [5/50], Loss: 1.1579
Iteration 1: Epoch [6/50], Loss: 1.0896
Iteration 1: Epoch [7/50], Loss: 1.0330
Iteration 1: Epoch [8/50], Loss: 0.9812
Iteration 1: Epoch [9/50], Loss: 0.9379
Iteration 1: Epoch [10/50], Loss: 0.8901
Iteration 1: Epoch [11/50], Loss: 0.8525
Iteration 1: Epoch [12/50], Loss: 0.8177
Iteration 1: Epoch [13/50], Loss: 0.7826
Iteration 1: Epoch [14/50], Loss: 0.7493
Iteration 1: Epoch [15/50], Loss: 0.7232
Iteration 1: Epoch [16/50], Loss: 0.6908
Iteration 1: Epoch [17/50], Loss: 0.6651
Iteration 1: Epoch [18/50], Loss: 0.6331
Iteration 1: Epoch [19/50], Loss: 0.6076
Iteration 1: Epoch [20/50], Loss: 0.5873
Iteration 1: Epoch [21/50], Loss: 0.5621
Iteration 1: Epoch [22/50], Loss: 0.5397
Iteration 1: Epoch [23/50], Loss: 0.5226
Iteration 1: Epoch [24/50], Loss: 0.5031
Iteration 1: Epoch [25/50], Loss: 0.4854
Iteration 1: Epoch [26/50], Loss: 0.4725
Iteration 1: Epoch [27/50], Loss: 0.4585
Iteration 1: Epoch [28/50], Loss: 0.4460
Iteration 1: Epoch [29/50], Loss: 0.4256
Iteration 1: Epoch [30/50], Loss: 0.4139
Iteration 1: Epoch [31/50], Loss: 0.4072
Iteration 1: Epoch [32/50], Loss: 0.3937
Iteration 1: Epoch [33/50], Loss: 0.3832
Iteration 1: Epoch [34/50], Loss: 0.3745
Iteration 1: Epoch [35/50], Loss: 0.3610
Iteration 1: Epoch [36/50], Loss: 0.3571
Iteration 1: Epoch [37/50], Loss: 0.3484
Iteration 1: Epoch [38/50], Loss: 0.3416
Iteration 1: Epoch [39/50], Loss: 0.3262
Iteration 1: Epoch [40/50], Loss: 0.3271
Iteration 1: Epoch [41/50], Loss: 0.3193
Iteration 1: Epoch [42/50], Loss: 0.3052
Iteration 1: Epoch [43/50], Loss: 0.3088
Iteration 1: Epoch [44/50], Loss: 0.2961
Iteration 1: Epoch [45/50], Loss: 0.2888
Iteration 1: Epoch [46/50], Loss: 0.2841
Iteration 1: Epoch [47/50], Loss: 0.2850
Iteration 1: Epoch [48/50], Loss: 0.2729
Iteration 1: Epoch [49/50], Loss: 0.2738
Iteration 1: Epoch [50/50], Loss: 0.2683
Loss on Last Iteration for Training is 167.69707826524973

Iteration 1: Testing on Test Set
Accuracy: 0.7146
Precision: 0.7140
Recall: 0.7146
F1 Score: 0.7128

Classification Report:

              precision    recall  f1-score   support

           0       0.70      0.80      0.75      1000
           1       0.85      0.80      0.82      1000
           2       0.64      0.55      0.59      1000
           3       0.55      0.50      0.52      1000
           4       0.66      0.66      0.66      1000
           5       0.61      0.64      0.62      1000
           6       0.71      0.84      0.77      1000
           7       0.77      0.78      0.77      1000
           8       0.84      0.82      0.83      1000
           9       0.81      0.76      0.79      1000

    accuracy                           0.71     10000
   macro avg       0.71      0.71      0.71     10000
weighted avg       0.71      0.71      0.71     10000

Iteration 2: Epoch [1/50], Loss: 1.8625
Iteration 2: Epoch [2/50], Loss: 1.4447
Iteration 2: Epoch [3/50], Loss: 1.2713
Iteration 2: Epoch [4/50], Loss: 1.1385
Iteration 2: Epoch [5/50], Loss: 1.0265
Iteration 2: Epoch [6/50], Loss: 0.9390
Iteration 2: Epoch [7/50], Loss: 0.8649
Iteration 2: Epoch [8/50], Loss: 0.7899
Iteration 2: Epoch [9/50], Loss: 0.7289
Iteration 2: Epoch [10/50], Loss: 0.6738
Iteration 2: Epoch [11/50], Loss: 0.6209
Iteration 2: Epoch [12/50], Loss: 0.5764
Iteration 2: Epoch [13/50], Loss: 0.5211
Iteration 2: Epoch [14/50], Loss: 0.4906
Iteration 2: Epoch [15/50], Loss: 0.4422
Iteration 2: Epoch [16/50], Loss: 0.4168
Iteration 2: Epoch [17/50], Loss: 0.3863
Iteration 2: Epoch [18/50], Loss: 0.3624
Iteration 2: Epoch [19/50], Loss: 0.3432
Iteration 2: Epoch [20/50], Loss: 0.3234
Iteration 2: Epoch [21/50], Loss: 0.2989
Iteration 2: Epoch [22/50], Loss: 0.2864
Iteration 2: Epoch [23/50], Loss: 0.2710
Iteration 2: Epoch [24/50], Loss: 0.2562
Iteration 2: Epoch [25/50], Loss: 0.2425
Iteration 2: Epoch [26/50], Loss: 0.2389
Iteration 2: Epoch [27/50], Loss: 0.2310
Iteration 2: Epoch [28/50], Loss: 0.2115
Iteration 2: Epoch [29/50], Loss: 0.2080
Iteration 2: Epoch [30/50], Loss: 0.2011
Iteration 2: Epoch [31/50], Loss: 0.1898
Iteration 2: Epoch [32/50], Loss: 0.1876
Iteration 2: Epoch [33/50], Loss: 0.1876
Iteration 2: Epoch [34/50], Loss: 0.1826
Iteration 2: Epoch [35/50], Loss: 0.1759
Iteration 2: Epoch [36/50], Loss: 0.1713
Iteration 2: Epoch [37/50], Loss: 0.1580
Iteration 2: Epoch [38/50], Loss: 0.1629
Iteration 2: Epoch [39/50], Loss: 0.1575
Iteration 2: Epoch [40/50], Loss: 0.1516
Iteration 2: Epoch [41/50], Loss: 0.1495
Iteration 2: Epoch [42/50], Loss: 0.1501
Iteration 2: Epoch [43/50], Loss: 0.1439
Iteration 2: Epoch [44/50], Loss: 0.1422
Iteration 2: Epoch [45/50], Loss: 0.1442
Iteration 2: Epoch [46/50], Loss: 0.1353
Iteration 2: Epoch [47/50], Loss: 0.1359
Iteration 2: Epoch [48/50], Loss: 0.1308
Iteration 2: Epoch [49/50], Loss: 0.1359
Iteration 2: Epoch [50/50], Loss: 0.1301
Loss on Last Iteration for Training is 81.29279561061412

Iteration 2: Testing on Test Set
Accuracy: 0.7484
Precision: 0.7533
Recall: 0.7484
F1 Score: 0.7490

Classification Report:

              precision    recall  f1-score   support

           0       0.75      0.81      0.78      1000
           1       0.86      0.85      0.86      1000
           2       0.68      0.60      0.64      1000
           3       0.53      0.62      0.57      1000
           4       0.66      0.75      0.70      1000
           5       0.71      0.58      0.64      1000
           6       0.81      0.81      0.81      1000
           7       0.86      0.78      0.82      1000
           8       0.86      0.84      0.85      1000
           9       0.81      0.84      0.82      1000

    accuracy                           0.75     10000
   macro avg       0.75      0.75      0.75     10000
weighted avg       0.75      0.75      0.75     10000

Iteration 3: Epoch [1/50], Loss: 1.9464
Iteration 3: Epoch [2/50], Loss: 1.5172
Iteration 3: Epoch [3/50], Loss: 1.3090
Iteration 3: Epoch [4/50], Loss: 1.1394
Iteration 3: Epoch [5/50], Loss: 1.0100
Iteration 3: Epoch [6/50], Loss: 0.9037
Iteration 3: Epoch [7/50], Loss: 0.8230
Iteration 3: Epoch [8/50], Loss: 0.7541
Iteration 3: Epoch [9/50], Loss: 0.7060
Iteration 3: Epoch [10/50], Loss: 0.6505
Iteration 3: Epoch [11/50], Loss: 0.6030
Iteration 3: Epoch [12/50], Loss: 0.5606
Iteration 3: Epoch [13/50], Loss: 0.5255
Iteration 3: Epoch [14/50], Loss: 0.4915
Iteration 3: Epoch [15/50], Loss: 0.4631
Iteration 3: Epoch [16/50], Loss: 0.4304
Iteration 3: Epoch [17/50], Loss: 0.4045
Iteration 3: Epoch [18/50], Loss: 0.3787
Iteration 3: Epoch [19/50], Loss: 0.3663
Iteration 3: Epoch [20/50], Loss: 0.3430
Iteration 3: Epoch [21/50], Loss: 0.3232
Iteration 3: Epoch [22/50], Loss: 0.3091
Iteration 3: Epoch [23/50], Loss: 0.2986
Iteration 3: Epoch [24/50], Loss: 0.2858
Iteration 3: Epoch [25/50], Loss: 0.2739
Iteration 3: Epoch [26/50], Loss: 0.2572
Iteration 3: Epoch [27/50], Loss: 0.2538
Iteration 3: Epoch [28/50], Loss: 0.2328
Iteration 3: Epoch [29/50], Loss: 0.2276
Iteration 3: Epoch [30/50], Loss: 0.2247
Iteration 3: Epoch [31/50], Loss: 0.2180
Iteration 3: Epoch [32/50], Loss: 0.2045
Iteration 3: Epoch [33/50], Loss: 0.2001
Iteration 3: Epoch [34/50], Loss: 0.1972
Iteration 3: Epoch [35/50], Loss: 0.1918
Iteration 3: Epoch [36/50], Loss: 0.1931
Iteration 3: Epoch [37/50], Loss: 0.1803
Iteration 3: Epoch [38/50], Loss: 0.1810
Iteration 3: Epoch [39/50], Loss: 0.1774
Iteration 3: Epoch [40/50], Loss: 0.1698
Iteration 3: Epoch [41/50], Loss: 0.1658
Iteration 3: Epoch [42/50], Loss: 0.1642
Iteration 3: Epoch [43/50], Loss: 0.1611
Iteration 3: Epoch [44/50], Loss: 0.1600
Iteration 3: Epoch [45/50], Loss: 0.1558
Iteration 3: Epoch [46/50], Loss: 0.1546
Iteration 3: Epoch [47/50], Loss: 0.1461
Iteration 3: Epoch [48/50], Loss: 0.1485
Iteration 3: Epoch [49/50], Loss: 0.1376
Iteration 3: Epoch [50/50], Loss: 0.1452
Loss on Last Iteration for Training is 90.73859457392246

Iteration 3: Testing on Test Set
Accuracy: 0.8170
Precision: 0.8177
Recall: 0.8170
F1 Score: 0.8162

Classification Report:

              precision    recall  f1-score   support

           0       0.78      0.90      0.84      1000
           1       0.90      0.92      0.91      1000
           2       0.77      0.70      0.73      1000
           3       0.68      0.62      0.64      1000
           4       0.77      0.82      0.79      1000
           5       0.73      0.76      0.75      1000
           6       0.84      0.87      0.85      1000
           7       0.85      0.88      0.86      1000
           8       0.94      0.86      0.90      1000
           9       0.91      0.85      0.88      1000

    accuracy                           0.82     10000
   macro avg       0.82      0.82      0.82     10000
weighted avg       0.82      0.82      0.82     10000

Iteration 4: Epoch [1/50], Loss: 1.9570
Iteration 4: Epoch [2/50], Loss: 1.5310
Iteration 4: Epoch [3/50], Loss: 1.3249
Iteration 4: Epoch [4/50], Loss: 1.1644
Iteration 4: Epoch [5/50], Loss: 1.0306
Iteration 4: Epoch [6/50], Loss: 0.9230
Iteration 4: Epoch [7/50], Loss: 0.8401
Iteration 4: Epoch [8/50], Loss: 0.7672
Iteration 4: Epoch [9/50], Loss: 0.7024
Iteration 4: Epoch [10/50], Loss: 0.6502
Iteration 4: Epoch [11/50], Loss: 0.6001
Iteration 4: Epoch [12/50], Loss: 0.5580
Iteration 4: Epoch [13/50], Loss: 0.5237
Iteration 4: Epoch [14/50], Loss: 0.4828
Iteration 4: Epoch [15/50], Loss: 0.4586
Iteration 4: Epoch [16/50], Loss: 0.4348
Iteration 4: Epoch [17/50], Loss: 0.3975
Iteration 4: Epoch [18/50], Loss: 0.3771
Iteration 4: Epoch [19/50], Loss: 0.3589
Iteration 4: Epoch [20/50], Loss: 0.3409
Iteration 4: Epoch [21/50], Loss: 0.3179
Iteration 4: Epoch [22/50], Loss: 0.3056
Iteration 4: Epoch [23/50], Loss: 0.2898
Iteration 4: Epoch [24/50], Loss: 0.2758
Iteration 4: Epoch [25/50], Loss: 0.2718
Iteration 4: Epoch [26/50], Loss: 0.2554
Iteration 4: Epoch [27/50], Loss: 0.2502
Iteration 4: Epoch [28/50], Loss: 0.2366
Iteration 4: Epoch [29/50], Loss: 0.2253
Iteration 4: Epoch [30/50], Loss: 0.2226
Iteration 4: Epoch [31/50], Loss: 0.2142
Iteration 4: Epoch [32/50], Loss: 0.2044
Iteration 4: Epoch [33/50], Loss: 0.2025
Iteration 4: Epoch [34/50], Loss: 0.1914
Iteration 4: Epoch [35/50], Loss: 0.1922
Iteration 4: Epoch [36/50], Loss: 0.1867
Iteration 4: Epoch [37/50], Loss: 0.1813
Iteration 4: Epoch [38/50], Loss: 0.1721
Iteration 4: Epoch [39/50], Loss: 0.1711
Iteration 4: Epoch [40/50], Loss: 0.1653
Iteration 4: Epoch [41/50], Loss: 0.1626
Iteration 4: Epoch [42/50], Loss: 0.1554
Iteration 4: Epoch [43/50], Loss: 0.1547
Iteration 4: Epoch [44/50], Loss: 0.1494
Iteration 4: Epoch [45/50], Loss: 0.1480
Iteration 4: Epoch [46/50], Loss: 0.1473
Iteration 4: Epoch [47/50], Loss: 0.1433
Iteration 4: Epoch [48/50], Loss: 0.1432
Iteration 4: Epoch [49/50], Loss: 0.1400
Iteration 4: Epoch [50/50], Loss: 0.1354
Loss on Last Iteration for Training is 84.62412586994469

Iteration 4: Testing on Test Set
Accuracy: 0.8268
Precision: 0.8294
Recall: 0.8268
F1 Score: 0.8258

Classification Report:

              precision    recall  f1-score   support

           0       0.76      0.92      0.83      1000
           1       0.92      0.93      0.92      1000
           2       0.74      0.75      0.75      1000
           3       0.68      0.68      0.68      1000
           4       0.81      0.79      0.80      1000
           5       0.85      0.66      0.74      1000
           6       0.85      0.88      0.86      1000
           7       0.83      0.89      0.86      1000
           8       0.90      0.92      0.91      1000
           9       0.95      0.85      0.90      1000

    accuracy                           0.83     10000
   macro avg       0.83      0.83      0.83     10000
weighted avg       0.83      0.83      0.83     10000

Iteration 5: Epoch [1/50], Loss: 2.0633
Iteration 5: Epoch [2/50], Loss: 1.6692
Iteration 5: Epoch [3/50], Loss: 1.4573
Iteration 5: Epoch [4/50], Loss: 1.2872
Iteration 5: Epoch [5/50], Loss: 1.1261
Iteration 5: Epoch [6/50], Loss: 0.9965
Iteration 5: Epoch [7/50], Loss: 0.8932
Iteration 5: Epoch [8/50], Loss: 0.8076
Iteration 5: Epoch [9/50], Loss: 0.7311
Iteration 5: Epoch [10/50], Loss: 0.6738
Iteration 5: Epoch [11/50], Loss: 0.6102
Iteration 5: Epoch [12/50], Loss: 0.5640
Iteration 5: Epoch [13/50], Loss: 0.5140
Iteration 5: Epoch [14/50], Loss: 0.4730
Iteration 5: Epoch [15/50], Loss: 0.4347
Iteration 5: Epoch [16/50], Loss: 0.3967
Iteration 5: Epoch [17/50], Loss: 0.3664
Iteration 5: Epoch [18/50], Loss: 0.3354
Iteration 5: Epoch [19/50], Loss: 0.3109
Iteration 5: Epoch [20/50], Loss: 0.2792
Iteration 5: Epoch [21/50], Loss: 0.2608
Iteration 5: Epoch [22/50], Loss: 0.2487
Iteration 5: Epoch [23/50], Loss: 0.2210
Iteration 5: Epoch [24/50], Loss: 0.2119
Iteration 5: Epoch [25/50], Loss: 0.2017
Iteration 5: Epoch [26/50], Loss: 0.1756
Iteration 5: Epoch [27/50], Loss: 0.1652
Iteration 5: Epoch [28/50], Loss: 0.1578
Iteration 5: Epoch [29/50], Loss: 0.1473
Iteration 5: Epoch [30/50], Loss: 0.1357
Iteration 5: Epoch [31/50], Loss: 0.1247
Iteration 5: Epoch [32/50], Loss: 0.1167
Iteration 5: Epoch [33/50], Loss: 0.1084
Iteration 5: Epoch [34/50], Loss: 0.1087
Iteration 5: Epoch [35/50], Loss: 0.1010
Iteration 5: Epoch [36/50], Loss: 0.0984
Iteration 5: Epoch [37/50], Loss: 0.0877
Iteration 5: Epoch [38/50], Loss: 0.0882
Iteration 5: Epoch [39/50], Loss: 0.0811
Iteration 5: Epoch [40/50], Loss: 0.0818
Iteration 5: Epoch [41/50], Loss: 0.0772
Iteration 5: Epoch [42/50], Loss: 0.0735
Iteration 5: Epoch [43/50], Loss: 0.0682
Iteration 5: Epoch [44/50], Loss: 0.0708
Iteration 5: Epoch [45/50], Loss: 0.0692
Iteration 5: Epoch [46/50], Loss: 0.0710
Iteration 5: Epoch [47/50], Loss: 0.0636
Iteration 5: Epoch [48/50], Loss: 0.0599
Iteration 5: Epoch [49/50], Loss: 0.0582
Iteration 5: Epoch [50/50], Loss: 0.0580
Loss on Last Iteration for Training is 36.22123565187212

Iteration 5: Testing on Test Set
Accuracy: 0.8200
Precision: 0.8252
Recall: 0.8200
F1 Score: 0.8188

Classification Report:

              precision    recall  f1-score   support

           0       0.83      0.84      0.83      1000
           1       0.91      0.93      0.92      1000
           2       0.80      0.72      0.76      1000
           3       0.76      0.56      0.64      1000
           4       0.70      0.89      0.78      1000
           5       0.71      0.81      0.75      1000
           6       0.83      0.88      0.86      1000
           7       0.85      0.85      0.85      1000
           8       0.97      0.83      0.89      1000
           9       0.90      0.90      0.90      1000

    accuracy                           0.82     10000
   macro avg       0.83      0.82      0.82     10000
weighted avg       0.83      0.82      0.82     10000

Overlap Table

Weaker 0 and Stronger 1
Label       % Weaker of Stronger     % Stronger of Weaker     
--------------------------------------------------------------
0           91.0                     92.6                     
1           89.79                    95.51                    
2           77.32                    84.91                    
3           68.22                    85.69                    
4           79.36                    89.7                     
5           87.84                    80.53                    
6           95.29                    91.33                    
7           91.62                    91.62                    
8           92.51                    94.53                    
9           86.79                    95.42                    
--------------------------------------------------------------
% Weaker of Stronger Total: 86.68
% Stronger of Weaker Total: 90.78

Weaker 0 and Stronger 2
Label       % Weaker of Stronger     % Stronger of Weaker     
--------------------------------------------------------------
0           85.52                    96.36                    
1           85.23                    98.0                     
2           70.71                    90.0                     
3           64.12                    79.64                    
4           73.05                    90.76                    
5           74.84                    89.17                    
6           90.67                    93.47                    
7           85.19                    96.39                    
8           89.78                    93.92                    
9           85.04                    94.5                     
--------------------------------------------------------------
% Weaker of Stronger Total: 81.27
% Stronger of Weaker Total: 92.92

Weaker 0 and Stronger 3
Label       % Weaker of Stronger     % Stronger of Weaker     
--------------------------------------------------------------
0           84.61                    97.24                    
1           84.16                    97.5                     
2           67.42                    92.18                    
3           60.86                    83.06                    
4           73.86                    88.64                    
5           78.28                    81.48                    
6           90.58                    94.77                    
7           83.86                    95.75                    
8           86.26                    96.84                    
9           85.04                    94.5                     
--------------------------------------------------------------
% Weaker of Stronger Total: 80.3
% Stronger of Weaker Total: 92.91

Weaker 0 and Stronger 4
Label       % Weaker of Stronger     % Stronger of Weaker     
--------------------------------------------------------------
0           87.01                    91.59                    
1           84.23                    97.38                    
2           67.22                    88.0                     
3           61.61                    69.56                    
4           70.19                    94.55                    
5           72.06                    91.52                    
6           89.59                    94.06                    
7           86.07                    93.94                    
8           88.93                    89.79                    
9           82.46                    96.6                     
--------------------------------------------------------------
% Weaker of Stronger Total: 79.8
% Stronger of Weaker Total: 91.58

Weaker 1 and Stronger 2
Label       % Weaker of Stronger     % Stronger of Weaker     
--------------------------------------------------------------
0           87.19                    96.55                    
1           90.55                    97.89                    
2           78.0                     90.4                     
3           79.38                    78.49                    
4           84.15                    92.49                    
5           72.07                    93.66                    
6           88.94                    95.66                    
7           85.76                    97.04                    
8           91.64                    93.82                    
9           92.82                    93.81                    
--------------------------------------------------------------
% Weaker of Stronger Total: 85.57
% Stronger of Weaker Total: 93.41

Weaker 1 and Stronger 3
Label       % Weaker of Stronger     % Stronger of Weaker     
--------------------------------------------------------------
0           85.81                    96.92                    
1           89.66                    97.65                    
2           74.6                     92.88                    
3           76.07                    82.66                    
4           84.22                    89.41                    
5           75.26                    85.45                    
6           87.63                    95.66                    
7           84.42                    96.39                    
8           88.53                    97.27                    
9           92.34                    93.33                    
--------------------------------------------------------------
% Weaker of Stronger Total: 84.45
% Stronger of Weaker Total: 93.29

Weaker 1 and Stronger 4
Label       % Weaker of Stronger     % Stronger of Weaker     
--------------------------------------------------------------
0           87.49                    90.51                    
1           89.31                    97.07                    
2           73.47                    87.58                    
3           78.04                    70.14                    
4           79.53                    94.77                    
5           67.99                    94.18                    
6           87.1                     95.42                    
7           86.07                    93.94                    
8           90.37                    89.3                     
9           89.5                     95.36                    
--------------------------------------------------------------
% Weaker of Stronger Total: 83.35
% Stronger of Weaker Total: 91.33

Weaker 2 and Stronger 3
Label       % Weaker of Stronger     % Stronger of Weaker     
--------------------------------------------------------------
0           94.54                    96.44                    
1           96.66                    97.39                    
2           85.37                    91.71                    
3           77.25                    84.9                     
4           91.54                    88.41                    
5           92.46                    80.76                    
6           93.87                    95.28                    
7           94.02                    94.87                    
8           91.88                    98.61                    
9           94.82                    94.82                    
--------------------------------------------------------------
% Weaker of Stronger Total: 91.68
% Stronger of Weaker Total: 92.78

Weaker 2 and Stronger 4
Label       % Weaker of Stronger     % Stronger of Weaker     
--------------------------------------------------------------
0           95.83                    89.53                    
1           96.22                    96.74                    
2           84.72                    87.14                    
3           79.46                    72.24                    
4           87.51                    94.88                    
5           86.77                    92.49                    
6           93.44                    95.16                    
7           95.51                    92.14                    
8           94.34                    91.06                    
9           91.84                    96.82                    
--------------------------------------------------------------
% Weaker of Stronger Total: 91.11
% Stronger of Weaker Total: 91.44

Weaker 3 and Stronger 4
Label       % Weaker of Stronger     % Stronger of Weaker     
--------------------------------------------------------------
0           96.66                    88.54                    
1           96.22                    96.01                    
2           89.03                    85.24                    
3           85.18                    70.46                    
4           85.94                    96.46                    
5           77.38                    94.42                    
6           95.02                    95.35                    
7           95.04                    90.86                    
8           96.99                    87.23                    
9           91.84                    96.82                    
--------------------------------------------------------------
% Weaker of Stronger Total: 91.26
% Stronger of Weaker Total: 90.51

/home/sebperre/venv-python/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/sebperre/venv-python/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Epoch 1, Loss: 0.685959976196289
Epoch 2, Loss: 0.4946022443294525
Epoch 3, Loss: 0.4321006428718567
Epoch 4, Loss: 0.3898250766038895
Epoch 5, Loss: 0.35809786059856413
Epoch 6, Loss: 0.32507630611658095
Epoch 7, Loss: 0.30431833480596543
Epoch 8, Loss: 0.27763792649507524
Epoch 9, Loss: 0.26048324173688886
Epoch 10, Loss: 0.23723952088356018
Epoch 11, Loss: 0.21557570455670358
Epoch 12, Loss: 0.20017698875665665
Epoch 13, Loss: 0.18578428563475607
Epoch 14, Loss: 0.16760773797631265
Epoch 15, Loss: 0.15564725220501424
/home/sebperre/programming-projects/efficient-inference-in-dl/iterative_models/cifar_vgg_efficient_inference.py:201: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
  all_preds.append(int(pred.cpu().numpy()))
/home/sebperre/programming-projects/efficient-inference-in-dl/iterative_models/cifar_vgg_efficient_inference.py:221: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
  all_preds.append(int(pred.cpu().numpy()))
Model Comparison

Statistic           Best                     Combined                 Difference               
-----------------------------------------------------------------------------------------------
Accuracy (%)        82.0                     81.44                    0.56                     
Precision (%)       82.5171                  81.703                   0.8141                   
Recall (%)          82.0                     81.44                    0.56                     
F1 (%)              81.8796                  81.2688                  0.6108                   
Time (s)            10.969                   57.5369                  -46.568                  
-----------------------------------------------------------------------------------------------

Best Model Class Report
              precision    recall  f1-score   support

           0       0.83      0.84      0.83      1000
           1       0.91      0.93      0.92      1000
           2       0.80      0.72      0.76      1000
           3       0.76      0.56      0.64      1000
           4       0.70      0.89      0.78      1000
           5       0.71      0.81      0.75      1000
           6       0.83      0.88      0.86      1000
           7       0.85      0.85      0.85      1000
           8       0.97      0.83      0.89      1000
           9       0.90      0.90      0.90      1000

    accuracy                           0.82     10000
   macro avg       0.83      0.82      0.82     10000
weighted avg       0.83      0.82      0.82     10000


Combined Model Class Report
              precision    recall  f1-score   support

           0       0.80      0.87      0.83      1000
           1       0.89      0.92      0.91      1000
           2       0.81      0.70      0.75      1000
           3       0.74      0.56      0.64      1000
           4       0.72      0.87      0.79      1000
           5       0.71      0.80      0.75      1000
           6       0.83      0.87      0.85      1000
           7       0.86      0.86      0.86      1000
           8       0.91      0.84      0.88      1000
           9       0.91      0.85      0.88      1000

    accuracy                           0.81     10000
   macro avg       0.82      0.81      0.81     10000
weighted avg       0.82      0.81      0.81     10000

